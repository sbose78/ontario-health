name: Ontario Health Data Ingestion

on:
  # Run on merge to main
  push:
    branches:
      - main
    paths:
      - 'pipeline/**'
      - 'sql/**'
      - '.github/workflows/weekly-ingest.yml'
  
  # Scheduled runs
  schedule:
    - cron: '0 11 * * 3'       # Wednesday 6am EST - wastewater
    - cron: '0 */3 * * *'      # Every 3 hours - ED wait times
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      dataset:
        description: 'Dataset to ingest'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - wastewater
          - ed_wait_times

env:
  PYTHON_VERSION: '3.11'

jobs:
  ingest:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r pipeline/requirements.txt

      - name: Create Snowflake private key file
        run: |
          mkdir -p ~/.snowflake
          echo "${{ secrets.SNOWFLAKE_PRIVATE_KEY }}" > ~/.snowflake/ontario_health_key.p8
          chmod 600 ~/.snowflake/ontario_health_key.p8

      - name: Determine dataset to ingest
        id: dataset
        run: |
          # Determine what to run based on trigger
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            # Manual trigger - use selected dataset
            echo "dataset=${{ inputs.dataset }}" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" == "push" ]; then
            # Merge to main - run all ingestors to refresh data
            echo "dataset=all" >> $GITHUB_OUTPUT
          elif [ "$(date +%u)" == "3" ] && [ "$(date +%H)" == "11" ]; then
            # Wednesday 6am - wastewater
            echo "dataset=wastewater" >> $GITHUB_OUTPUT
          else
            # Other scheduled times - ED wait times
            echo "dataset=ed_wait_times" >> $GITHUB_OUTPUT
          fi

      - name: Run ingestion
        run: |
          cd pipeline
          python run_ingestion.py ${{ steps.dataset.outputs.dataset }}

      - name: Sync to D1 cache (for public dashboard)
        run: |
          cd pipeline
          python sync_to_d1.py

      - name: Verify data freshness
        run: |
          cd pipeline
          python -c "
          from config import get_snowflake_connection
          conn = get_snowflake_connection()
          cur = conn.cursor()
          cur.execute('SELECT * FROM MARTS_OPS.rpt_data_freshness')
          print('Data Freshness:')
          for row in cur.fetchall():
              print(f'  {row[0]}: {row[2]} ({row[4]:,} records)')
          cur.close()
          conn.close()
          "

